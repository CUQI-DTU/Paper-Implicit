{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson problem (1D/2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuqi.problem import BayesianProblem\n",
    "from cuqi.distribution import Gaussian\n",
    "from cuqipy_fenics.utilities import ExpressionFromCallable\n",
    "from cuqipy_fenics.testproblem import FEniCSPoisson2D, FEniCSDiffusion1D\n",
    "from cuqi.experimental.mcmc import MH, NUTS, MALA, ULA \n",
    "import numpy as np\n",
    "import cuqi\n",
    "import cuqipy_fenics\n",
    "import dolfin as dl\n",
    "import matplotlib.pyplot as plt\n",
    "# import check_grad\n",
    "from scipy.optimize import check_grad\n",
    "from cuqi.utilities import approx_gradient\n",
    "\n",
    "# set logging level of dl\n",
    "dl.set_log_level(dl.LogLevel.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print CUQIpy and CUQIpy-FEniCS versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cuqi.__version__)\n",
    "print(cuqipy_fenics.__version__)\n",
    "print(cuqipy_fenics.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_type = 'diffusion_2D'  #'convolution' #'convolution' #'diffusion' # 'identity' # choice of model type\n",
    "prior_type = 'TV'#'Gaussian' #'TV' #'Gaussian' # 'TV'\n",
    "enable_FD = False\n",
    "#sampler_choice = \"NUTS\" #\"MALA\" #\"NUTS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter set up\n",
    "if model_type == 'convolution':\n",
    "    np.random.seed(0)\n",
    "    sampler_choice = \"NUTS\"\n",
    "    TV_weight = 1.0e2\n",
    "    TV_beta = 1e-4\n",
    "    plot_par = False\n",
    "    Ns = 1\n",
    "    Nb = 500\n",
    "    max_depth = 5\n",
    "    step_size = None\n",
    "    remove_burnin = 0\n",
    "    apply_grid_search=False\n",
    "    # initial point was zero\n",
    "\n",
    "    # these variables are not used in the deconvolution case\n",
    "    source_coeff = 5\n",
    "    sample_prior = False\n",
    "\n",
    "if model_type == 'diffusion':\n",
    "    np.random.seed(0)\n",
    "    sampler_choice = \"ULA\"\n",
    "    TV_weight = 35 #40 #80 #40 #60\n",
    "    TV_beta = 1e-7\n",
    "    plot_par = True\n",
    "    Ns = 5*20000 #1000\n",
    "    Nb = 10 #20000\n",
    "    max_depth = 7\n",
    "    step_size = None\n",
    "    remove_burnin = 3000 #0#int(Nb/2) #15000\n",
    "    apply_grid_search=False\n",
    "    signal_type = 'square'\n",
    "    set_x0_map_tobe_true = False\n",
    "    run_scipy_with_callback = False\n",
    "    noise_level = 0.01\n",
    "    source_coeff = 1\n",
    "    physical_dim = 1 # choice of physical dimension\n",
    "    sample_prior = False\n",
    "    use_DG_1 = True\n",
    "\n",
    "if model_type == 'diffusion_2D':\n",
    "    np.random.seed(0)\n",
    "    sampler_choice = \"NUTS\"\n",
    "    TV_weight = 60 #35 #40 #80 #40 #60\n",
    "    TV_beta = 1e-6 #1e-7\n",
    "    plot_par = True\n",
    "    Ns = 2#5*20000 #1000\n",
    "    Nb = 1 #20000\n",
    "    max_depth = 7\n",
    "    step_size = 0.005\n",
    "    remove_burnin = 0 # 3000 #0#int(Nb/2) #15000\n",
    "    apply_grid_search=False\n",
    "    signal_type = 'square'\n",
    "    set_x0_map_tobe_true = False\n",
    "    run_scipy_with_callback = False\n",
    "    noise_level = 0.01\n",
    "    source_coeff = 1\n",
    "    physical_dim = 2 # choice of physical dimension\n",
    "    sample_prior = False\n",
    "    use_DG_1 = True\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if model_type == 'diffusion':\n",
    "    endpoint = 10 # domain length\n",
    "    # f is sum of 3 exponentials at 1/4*endpoint and 1/2*endpoint and 3/4*endpoint\n",
    "    \n",
    "    def f(x):\n",
    "        return np.exp(-source_coeff*(x-1/4*endpoint)**2) + np.exp(-source_coeff*(x-1/2*endpoint)**2) + np.exp(-source_coeff*(x-3/4*endpoint)**2)\n",
    "        #return np.ones(len(x))\n",
    "    \n",
    "    #plot f\n",
    "    x_grid = np.linspace(0, endpoint, 100)\n",
    "    plt.plot(x_grid, f(x_grid))\n",
    "    \n",
    "    \n",
    "    f_expr = ExpressionFromCallable(f)\n",
    "elif model_type == 'diffusion_2D':\n",
    "    f_expr = dl.Constant(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'diffusion_2D':\n",
    "    \n",
    "    A = FEniCSPoisson2D(dim=(10,10), field_type=None, mapping='exponential', bc_types=['Dirichlet', 'Neumann', 'Dirichlet', 'Neumann']).model\n",
    "elif model_type == 'diffusion':\n",
    "        A = FEniCSDiffusion1D(dim=40, right_bc=0, f=f_expr, endpoint=endpoint).model # note source term is zero for the 1D case\n",
    "\n",
    "elif model_type == 'identity':\n",
    "    domain_dim = 41\n",
    "    A = cuqi.model.LinearModel(np.eye(domain_dim))\n",
    "\n",
    "elif model_type == 'convolution':\n",
    "    domain_dim = 40\n",
    "    A = cuqi.testproblem.Deconvolution1D(dim=domain_dim, PSF_param=2, PSF_size=40).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G_range = A.range_geometry\n",
    "\n",
    "if use_DG_1 == True:\n",
    "    H = dl.FunctionSpace(A.domain_geometry.mesh, 'DG', 1)\n",
    "    G_domain = cuqipy_fenics.geometry.FEniCSContinuous(H)\n",
    "    A.domain_geometry = G_domain\n",
    "else:\n",
    "    G_domain = A.domain_geometry\n",
    "    H = G_domain.function_space\n",
    "\n",
    "\n",
    "if model_type == 'diffusion' or model_type == 'diffusion_2D':\n",
    "    mesh = H.mesh()\n",
    "elif model_type == 'identity' or model_type == 'convolution':\n",
    "    mesh = dl.UnitIntervalMesh(domain_dim-1)\n",
    "    H = dl.FunctionSpace(mesh, 'CG', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H_DG = A.domain_geometry.function_space\n",
    "#updated_domain_geometry = cuqipy_fenics.geometry.FEniCSContinuous(H_DG)\n",
    "#A.domain_geometry = updated_domain_geometry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMPrior class\n",
    "class SMPrior:\n",
    "    def __init__(self, ginv, corrlength, var, mean, covariancetype=None):\n",
    "        self.corrlength = corrlength\n",
    "        self.mean = mean\n",
    "        self.c = 1e-9  # default value\n",
    "        if covariancetype is not None:\n",
    "            self.covariancetype = covariancetype\n",
    "        else:\n",
    "            self.covariancetype = 'Squared Distance'  # default\n",
    "        self.compute_L(ginv, corrlength, var)\n",
    "\n",
    "    def compute_L(self, g, corrlength, var):\n",
    "        ng = g.shape[0]\n",
    "        a = var - self.c\n",
    "        b = np.sqrt(-corrlength**2 / (2 * np.log(0.01)))\n",
    "        Gamma_pr = np.zeros((ng, ng))\n",
    "\n",
    "        for ii in range(ng):\n",
    "            for jj in range(ii, ng):\n",
    "                dist_ij = np.linalg.norm(g[ii, :] - g[jj, :])\n",
    "                if self.covariancetype == 'Squared Distance':\n",
    "                    gamma_ij = a * np.exp(-dist_ij**2 / (2 * b**2))\n",
    "                elif self.covariancetype == 'Ornstein-Uhlenbeck':\n",
    "                    gamma_ij = a * np.exp(-dist_ij / corrlength)\n",
    "                else:\n",
    "                    raise ValueError('Unrecognized prior covariance type')\n",
    "                if ii == jj:\n",
    "                    gamma_ij = gamma_ij + self.c\n",
    "                Gamma_pr[ii, jj] = gamma_ij\n",
    "                Gamma_pr[jj, ii] = gamma_ij\n",
    "        \n",
    "        self.cov = Gamma_pr\n",
    "        self.L = np.linalg.cholesky(np.linalg.inv(Gamma_pr)).T\n",
    "\n",
    "    def draw_samples(self, nsamples):\n",
    "        samples = self.mean + np.linalg.solve(self.L, np.random.randn(self.L.shape[0], nsamples))\n",
    "        return samples\n",
    "\n",
    "    def eval_fun(self, args):\n",
    "        sigma = args[0]\n",
    "        res = 0.5 * np.linalg.norm(self.L @ (sigma - self.mean))**2\n",
    "        return res\n",
    "    \n",
    "    def evaluate_target_external(self, x, compute_grad=False):\n",
    "        x = x.reshape((-1,1))\n",
    "        # print(\"x.shape: \", x.shape)\n",
    "        # print(\"self.mean.shape: \", self.mean.shape)\n",
    "        if compute_grad:\n",
    "            grad = self.L.T @ self.L @ (x - self.mean)\n",
    "        else:\n",
    "            grad = None\n",
    "        \n",
    "        return self.eval_fun(x), grad\n",
    "        \n",
    "\n",
    "    def compute_hess_and_grad(self, args, nparam):\n",
    "        sigma = args[0]\n",
    "        Hess = self.L.T @ self.L\n",
    "        grad = Hess @ (sigma - self.mean)\n",
    "\n",
    "        if nparam > len(sigma):\n",
    "            Hess = np.block([[Hess, np.zeros((len(sigma), nparam - len(sigma)))],\n",
    "                             [np.zeros((nparam - len(sigma), len(sigma))), np.zeros((nparam - len(sigma), nparam - len(sigma)))]])\n",
    "            grad = np.concatenate([grad, np.zeros(nparam - len(sigma))])\n",
    "\n",
    "\n",
    "        return Hess, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MyTV:\n",
    "#    def __init__(self, q0fun, mesh, delta,**kwargs):\n",
    "#        #self.qfun = project(qFunction(phi,q1,q2),V1)\n",
    "#\n",
    "#        self.V1 = FunctionSpace(mesh,'CG',1)\n",
    "#        self.V02 = VectorFunctionSpace(mesh,'DG',0)\n",
    "#\n",
    "#        self.q0fun = q0fun\n",
    "#        self.q0grad = project(grad(self.q0fun),self.V02)\n",
    "#        self.q0_denom = Denom(self.q0grad,delta)\n",
    "#\n",
    "#        # operator\n",
    "#        self.p_trial = TrialFunction(self.V1)\n",
    "#        self.p_test = TestFunction(self.V1)\n",
    "#\n",
    "#        #self.L_op = assemble(ufl.inner(self.p_trial, self.p_test)*dx)\n",
    "#        #self.TV_op = assemble(self.q_denom*ufl.inner(grad(self.p_trial),grad(self.p_test))*dx)\n",
    "#        self.TV_op = assemble((self.q0_denom*inner(grad(self.p_trial),grad(self.p_test)))*dx)\n",
    "#\n",
    "#        self.delta = delta\n",
    "#\n",
    "#    def eval_TV(self,qfun):\n",
    "#        self.update_op(qfun)\n",
    "#        return np.dot(self.TV_op * qfun.vector(),qfun.vector())\n",
    "#\n",
    "#    def eval_grad(self,qfun):\n",
    "#        self.update_op(qfun)\n",
    "#        return 2*(self.TV_op * qfun.vector())#[idx2]\n",
    "#    \n",
    "#    def update_op(self,q0fun):\n",
    "#        self.q0fun = q0fun\n",
    "#        self.q0grad = project(grad(self.q0fun),self.V02)\n",
    "#        self.q0_denom = Denom(self.q0grad,self.delta)\n",
    "#        self.TV_op = assemble((self.q0_denom*inner(grad(self.p_trial),grad(self.p_test)))*dx) \n",
    "#\n",
    "\n",
    "class TV_reg:\n",
    "    def __init__(self, V, beta, weight):\n",
    "        self.beta    = dl.Constant(beta)\n",
    "        self.m_tilde  = dl.TestFunction(V)\n",
    "        self.m_hat = dl.TrialFunction(V)\n",
    "        self.weight = weight\n",
    "        \n",
    "    def cost_reg(self, m):\n",
    "        return -self.weight*dl.assemble(dl.sqrt( dl.inner(dl.grad(m), dl.grad(m)) + self.beta)*dl.dx)\n",
    "\n",
    "    \n",
    "    def grad_reg(self, m):  \n",
    "        #print(\"grad\", dl.grad)      \n",
    "        TVm = dl.sqrt( dl.inner(dl.grad(m), dl.grad(m)) + self.beta)\n",
    "        self.TVm = TVm\n",
    "        grad_val = dl.assemble(dl.Constant(1.)/TVm*dl.inner(dl.grad(m), dl.grad(self.m_tilde))*dl.dx)\n",
    "        return -self.weight*grad_val\n",
    "    \n",
    "    def eval_density(self, x):\n",
    "        # convert from numpy array to dolfin function\n",
    "        x_fun = dl.Function(H)\n",
    "        x_fun.vector()[:] = x\n",
    "        return self.cost_reg(x_fun)\n",
    "    \n",
    "    def eval_grad_density(self, x):\n",
    "        # convert from numpy array to dolfin function\n",
    "        x_fun = dl.Function(H)\n",
    "        x_fun.vector()[:] = x\n",
    "        grad_val = self.grad_reg(x_fun).get_local()\n",
    "        return grad_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prior_type == 'Gaussian':\n",
    "\n",
    "    # Create correlation matrix for prior x\n",
    "    v2d = dl.vertex_to_dof_map(H)\n",
    "    d2v = dl.dof_to_vertex_map(H)\n",
    "    \n",
    "    mean_sigma = np.zeros((H.dim(), 1)) #linearization point\n",
    "    corrlength =  0.2* endpoint\n",
    "    var_sigma = 0.05 ** 2   #prior variance\n",
    "    \n",
    "    smprior = SMPrior(mesh.coordinates()[d2v], corrlength, var_sigma, mean_sigma)#, covariancetype='Ornstein-Uhlenbeck')\n",
    "    \n",
    "    \n",
    "    sample = smprior.draw_samples(1)\n",
    "    fun = dl.Function(H)\n",
    "    fun.vector().set_local(sample)\n",
    "    \n",
    "    im = dl.plot(fun)\n",
    "    if physical_dim == 2:\n",
    "        plt.colorbar(im)\n",
    "\n",
    "    # visualize the covariance matrix \n",
    "    plt.figure()\n",
    "    im = plt.imshow(smprior.cov)\n",
    "    plt.colorbar(im)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_reg = TV_reg(H, beta=TV_beta, weight=TV_weight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prior_type == 'Gaussian':\n",
    "    cov_const = 1 #100\n",
    "    x = Gaussian(np.zeros(G_domain.par_dim), cov=cov_const*smprior.cov, geometry=G_domain)\n",
    "elif prior_type == 'TV':\n",
    "    # CUQIpy user defined distribution\n",
    "    x = cuqi.distribution.UserDefinedDistribution(logpdf_func=tv_reg.eval_density,\n",
    "                                              gradient_func=tv_reg.eval_grad_density,\n",
    "                                              dim=H.dim())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_1 = np.random.randn(G_domain.par_dim)\n",
    "# x_1 step function\n",
    "x_1 = np.zeros(H.dim())\n",
    "x_1[G_domain.par_dim//2:] = 1\n",
    "x_1_cuqiarray = cuqi.array.CUQIarray(x_1)\n",
    "x_1_cuqiarray.plot()\n",
    "plt.figure()\n",
    "# verify the TV gradient computation\n",
    "plt.plot(x.gradient(x_1))\n",
    "# plot finite difference gradient \n",
    "\n",
    "plt.plot(approx_gradient(x.logd, x_1, 1e-12), '--')\n",
    "plt.legend(['TV class', 'finite difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_prior:\n",
    "    # sample the prior \n",
    "    Ns_prior = 1\n",
    "    if prior_type == 'Gaussian':\n",
    "        x_samples = x.sample(Ns_prior)\n",
    "    \n",
    "    elif prior_type == 'TV':\n",
    "        prior_sampler = NUTS(x, max_depth=5)\n",
    "        prior_sampler.warmup(1)\n",
    "        prior_sampler.sample(Ns_prior)\n",
    "        x_samples = prior_sampler.get_samples()\n",
    "    \n",
    "    #x_samples.plot()\n",
    "    \n",
    "    x_samples.plot_trace()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_prior:\n",
    "    x_samples.compute_ess()\n",
    "    #x_samples.burnthin(5).plot_ci()\n",
    "    plt.figure()\n",
    "    x_samples.plot([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'diffusion':\n",
    "    if prior_type == 'Gaussian':\n",
    "        x_true = x.sample()\n",
    "    elif prior_type == 'TV': \n",
    "        if signal_type == 'step':\n",
    "            fun_x_true_expr = dl.Expression('x[0] > 5 ? 0 : -0.5', degree=1)\n",
    "\n",
    "        elif signal_type == 'square':\n",
    "            fun_x_true_expr = dl.Expression('(x[0]>3.5 && x[0]< 6.5) ? 0 : -0.5', degree=1)\n",
    "\n",
    "        x_true_fun = dl.interpolate(fun_x_true_expr, H)\n",
    "        x_true = cuqi.array.CUQIarray(x_true_fun.vector().get_local(), geometry=A.domain_geometry)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Unrecognized prior type')\n",
    "    \n",
    "elif model_type == 'identity' or model_type == 'convolution':\n",
    "    x_true = cuqi.array.CUQIarray(np.zeros(G_domain.par_dim), geometry=A.domain_geometry)\n",
    "    x_true[G_domain.par_dim//2:] = 1 + np.random.randn(G_domain.par_dim//2)*0.1\n",
    "\n",
    "elif model_type == 'diffusion_2D':\n",
    "    # set a 2D signal with a square in the middle\n",
    "    fun_x_true_expr = dl.Expression('(x[0]>0.35 && x[0]< 0.65 && x[1]>0.35 && x[1]< 0.65) ? 0 : -0.5', degree=1)\n",
    "    x_true_fun = dl.interpolate(fun_x_true_expr, H)\n",
    "    x_true = cuqi.array.CUQIarray(x_true_fun.vector().get_local(), geometry=A.domain_geometry)\n",
    "\n",
    "\n",
    "\n",
    "im = x_true.plot()\n",
    "if physical_dim == 2:\n",
    "    plt.colorbar(im[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell to compute s_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.dim()\n",
    "G_domain.par_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.range_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_true.shape\n",
    "A.pde._parameter = dl.Function(H)\n",
    "y_true = A(x_true)\n",
    "s_noise = 1.0/np.sqrt(G_range.par_dim)* noise_level*np.linalg.norm(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Gaussian(A(x), s_noise**2, geometry=G_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = y(x=x_true).sample()\n",
    "y_obs.plot()\n",
    "y_true.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP = BayesianProblem(y, x).set_data(y=y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_1, label='x_1')\n",
    "\n",
    "x_2 = np.random.randn(G_domain.par_dim)\n",
    "plt.plot(x_2, label='x_2')\n",
    "plt.legend()\n",
    "#print(x.logd(x_1))\n",
    "#print(x.logd(x_1*0+1))\n",
    "#print(x.logd(np.random.randn(G_domain.par_dim)))\n",
    "\n",
    "# at x_1 , logd for prior and for likelihood\n",
    "print('x.logd(x_1)', x.logd(x_1))\n",
    "print('BP.likelihood.logd(x_1)', BP.likelihood.logd(x_1))\n",
    "\n",
    "# at x_1 , logd for prior and for likelihood\n",
    "print('x.logd(0.5*x_1)', x.logd(x_1*0.5))\n",
    "print('BP.likelihood.logd(0.5*x_1)', BP.likelihood.logd(x_1*0.5))\n",
    "\n",
    "# at x_2 , logd for prior and for likelihood\n",
    "print('x.logd(x_2)', x.logd(x_2))\n",
    "print('BP.likelihood.logd(x_2)', BP.likelihood.logd(x_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step function on space H\n",
    "fenics_exp = dl.Expression('x[1] > 0.5 ? 0 : -0.5', degree=1)\n",
    "fenics_fun = dl.interpolate(fenics_exp, H)\n",
    "step_array = fenics_fun.vector().get_local()\n",
    "print(x.logd(step_array))\n",
    "\n",
    "fenics_exp2 = dl.Expression('x[1] > 0.5 ? 0 : -0.5', degree=3)\n",
    "fenics_fun2 = dl.interpolate(fenics_exp2, H)\n",
    "step_array2 = fenics_fun2.vector().get_local()\n",
    "print(x.logd(step_array2))\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "grad_fun_step = dl.Function(H)\n",
    "grad_fun_step.vector()[:] = x.gradient(step_array2)\n",
    "im = dl.plot(grad_fun_step)\n",
    "plt.colorbar(im)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# plot fenics_fun2\n",
    "im = dl.plot(fenics_fun2)\n",
    "plt.colorbar(im)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.gradient(step_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pcolor of the gradient of the step function \n",
    "x_linspace = np.linspace(0, 1, 100)\n",
    "y_linspace = np.linspace(0, 1, 100)\n",
    "X, Y = np.meshgrid(x_linspace, y_linspace)\n",
    "Z = np.zeros((100, 100))\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        Z[i, j] = grad_fun_step(X[i, j], Y[i, j])\n",
    "\n",
    "plt.figure()\n",
    "im = plt.pcolor(X, Y, Z)\n",
    "plt.colorbar(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.plot(mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_1.shape)\n",
    "#BP.likelihood.logd(x_1)\n",
    "#BP.prior.logd(x_1)\n",
    "#BP.posterior.logd(x_1)\n",
    "#BP.posterior.gradient(x_1)\n",
    "\n",
    "BP.model.pde.parameter_function_space = H\n",
    "BP.likelihood.gradient(x_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map point\n",
    "if enable_FD:\n",
    "    BP.posterior.enable_FD() \n",
    "else:\n",
    "    BP.posterior.disable_FD()\n",
    "map_x = BP.MAP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot map predicted data\n",
    "\n",
    "im = y_obs.plot(label='y_obs')\n",
    "plt.colorbar(im[0])\n",
    "\n",
    "plt.figure()\n",
    "im = A(map_x).plot(label='A(x_map)')\n",
    "plt.colorbar(im[0])\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot of two columns and one row\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.sca(axs[0])\n",
    "im = map_x.plot(subplots=False, vmin=-0.6, vmax=0.1, mode='color')\n",
    "if physical_dim == 2:\n",
    "    # min value of the colorbar is set to -1.0\n",
    "    plt.colorbar(im[0])#, ticks=range(-1, 0, 5))\n",
    "\n",
    "plt.sca(axs[1])\n",
    "\n",
    "im = x_true.plot(subplots=False, vmin=-0.6, vmax=0.1, mode='color')\n",
    "plt.legend(['MAP', 'True'])\n",
    "if physical_dim == 2:\n",
    "    plt.colorbar(im[0])\n",
    "else:\n",
    "    plt.ylim(-1.0, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_x0_map_tobe_true:\n",
    "    x0_map = x_true\n",
    "else:\n",
    "    x0_map = x_true*0+0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_map.plot()\n",
    "x0_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy solver BFGS\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs\n",
    "fmin_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin_bfgs\n",
    "if run_scipy_with_callback:\n",
    "    Nfeval = 1\n",
    "    \n",
    "    \n",
    "    def callbackF(Xi):\n",
    "        global Nfeval\n",
    "        # plot Xi every 10 iterations\n",
    "        if Nfeval % 10 == 0:\n",
    "            plt.plot(Xi)\n",
    "            # print function value and gradient norm\n",
    "            print(\"{0:4d}   {1: 3.6f}   {2: 3.6f}   \".format(Nfeval, \n",
    "                                                            (-BP.posterior.logd(Xi)).to_numpy()[0],\n",
    "                                                            np.linalg.norm(- BP.posterior.gradient(Xi))))\n",
    "            \n",
    "    \n",
    "           #print(\"{0:4d}   {1: 3.6f}   {2: 3.6f}   {3: 3.6f}   {4: 3.6f}\".format(Nfeval, Xi[0], Xi[1], Xi[2], rosen(Xi)))\n",
    "        \n",
    "        #print(\"{0:4d}   {1: 3.6f}   {2: 3.6f}   {3: 3.6f}   {4: 3.6f}\".format(Nfeval, Xi[0], Xi[1], Xi[2], rosen(Xi)))\n",
    "        Nfeval += 1\n",
    "    \n",
    "    print('{0:4s}   {1:9s}   {2:9s}   {3:9s}   {4:9s}'.format('Iter', ' X1', ' X2', ' X3', 'f(X)')  ) \n",
    "    x0 = np.array([1.1, 1.1, 1.1], dtype=np.double)\n",
    "    [xopt, fopt, gopt, Bopt, func_calls, grad_calls, warnflg] = \\\n",
    "        fmin_bfgs(lambda x: -BP.posterior.logd(x),\n",
    "                  x0_map*0,\n",
    "                  #fprime=lambda x : -BP.posterior.gradient(x), \n",
    "                  callback=callbackF, \n",
    "                  disp=1,\n",
    "                  maxiter=2000, \n",
    "                  full_output=True, \n",
    "                  retall=False,\n",
    "                  gtol=5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(xopt)\n",
    "#plt.plot(x_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(xopt)\n",
    "#plt.plot(x_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(xopt, fopt, gopt, Bopt, func_calls, grad_calls, warnflg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TV_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35, 1e-7\n",
    "if apply_grid_search:\n",
    "    # grid search for computing the map\n",
    "    grid_weights = [20]\n",
    "    grid_beta = [1e-6] #1e-3, 1e-4, \n",
    "    map_list = []\n",
    "    weight_list = []\n",
    "    beta_list = []\n",
    "    TV_m_list = []\n",
    "    \n",
    "    for i, weight in enumerate(grid_weights):\n",
    "        for j, beta in enumerate(grid_beta):\n",
    "            print('iteration number {} of total {}'.format(i*len(grid_beta) + j, len(grid_weights)*len(grid_beta)))\n",
    "            print('weight: ', weight)\n",
    "            print('beta: ', beta)\n",
    "            TV_reg_i = TV_reg(H, beta=beta, weight=weight)\n",
    "            # define x\n",
    "            x = cuqi.distribution.UserDefinedDistribution(logpdf_func=TV_reg_i.eval_density,\n",
    "                                                        gradient_func=TV_reg_i.eval_grad_density,\n",
    "                                                        dim=H.dim())\n",
    "            BP_i = BayesianProblem(y, x).set_data(y=y_obs)\n",
    "            BP_i.posterior.disable_FD()\n",
    "            map_x = BP_i.MAP(x0=x0_map)\n",
    "            TV_m_list.append(TV_reg_i.TVm)\n",
    "            map_list.append(map_x)\n",
    "            weight_list.append(weight)\n",
    "            beta_list.append(beta)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_grid_search:\n",
    "    if physical_dim == 1:\n",
    "        # plot all map points with labels\n",
    "        style = ['-', '--', '-.', ':']*3\n",
    "        starting_idx = 0\n",
    "        for i, map_x_i in enumerate(map_list[starting_idx:starting_idx+3]):\n",
    "            map_x_i.plot(label='weight: '+str(weight_list[starting_idx+i])+' beta: '+str(beta_list[starting_idx+i]) + ' TVm: '+str(dl.assemble(TV_m_list[starting_idx+i]*dl.dx)), linestyle=style[i])\n",
    "            # print(map_x_i.info)\n",
    "        \n",
    "        x_true.plot(label='True', color='black')\n",
    "        plt.ylim(-0.6, 0.1)\n",
    "        #plot f\n",
    "\n",
    "        plt.plot(x_grid, f(x_grid)-0.5)\n",
    "\n",
    "        # plot legend and put it outside the plot\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    elif physical_dim == 2:\n",
    "\n",
    "        # create a subplot of two columns and one row\n",
    "        fig, axs = plt.subplots(len(grid_weights), len(grid_beta), figsize=(15, 15))\n",
    "        for i in range(len(grid_weights)):\n",
    "            for j in range(len(grid_beta)):\n",
    "                plt.sca(axs[i, j])\n",
    "                im = map_list[i*len(grid_beta) + j].plot(subplots=False, vmin=-0.6, vmax=0.1, mode='color')\n",
    "                plt.title('weight: '+str(weight_list[i*len(grid_beta) + j])+' beta: '+str(beta_list[i*len(grid_beta) + j]) + ' TVm: '+str(dl.assemble(TV_m_list[i*len(grid_beta) + j]*dl.dx)))\n",
    "                plt.colorbar(im[0])\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Unrecognized physical dimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x0_map.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_x.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posterior_samples = BP.UQ(Ns=3000, Nb=2, percent=97)\n",
    "posterior = BP.posterior\n",
    "#sampler = cuqi.sampler.NUTS(posterior, max_depth=4, x0=np.zeros(G_domain.par_dim)+.001)\n",
    "#sampler = cuqi.sampler.ULA(posterior, x0=np.zeros(G_domain.par_dim)+.001, scale=0.00000005)\n",
    "# np.zeros(G_domain.par_dim)+.001\n",
    "\n",
    "#initial_point = np.zeros(G_domain.par_dim) # x_true.to_numpy()\n",
    "#initial_point = 0.5*map_x.to_numpy()\n",
    "#initial_point = map_x.to_numpy()\n",
    "initial_point = np.ones(G_domain.par_dim)*-0.25\n",
    "\n",
    "#sampler = cuqi.experimental.mcmc.PCN(posterior, initial_point=initial_point, scale=0.05)#, max_depth=10, scale=0.005)\n",
    "\n",
    "if sampler_choice == \"NUTS\":\n",
    "    sampler = cuqi.experimental.mcmc.NUTS(posterior,\n",
    "                                          initial_point=initial_point,\n",
    "                                          max_depth=max_depth,\n",
    "                                          step_size=step_size)\n",
    "\n",
    "elif sampler_choice == \"MALA\":\n",
    "    sampler = cuqi.experimental.mcmc.MALA(posterior, initial_point=initial_point, scale=2.1e-7)\n",
    "    Ns = 1000\n",
    "    Nb = 20\n",
    "\n",
    "elif sampler_choice == \"ULA\":\n",
    "    sampler = cuqi.experimental.mcmc.ULA(posterior, initial_point=initial_point, scale=2.1e-7)\n",
    "\n",
    "\n",
    "elif sampler_choice == \"MH\":\n",
    "    sampler = cuqi.experimental.mcmc.MH(posterior, initial_point=initial_point, scale=2.1e-5)\n",
    "    Ns = 1000\n",
    "    Nb = 20\n",
    "\n",
    "\n",
    "_ = sampler.warmup(Nb)\n",
    "_ = sampler.sample(Ns)\n",
    "\n",
    "\n",
    "#posterior_samples = sampler.sample_adapt(100, Nb=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posterior_samples = sampler.get_samples().burnthin(remove_burnin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(sampler.scale)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.plot(sampler.num_tree_node_list)\n",
    "    plt.figure()\n",
    "    plt.semilogy(sampler.epsilon_list)\n",
    "    plt.semilogy(sampler.epsilon_bar_list)\n",
    "    print(np.max(sampler.epsilon_list))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler.scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code cell for generating Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if model_type == 'diffusion' or model_type == 'diffusion_2D':\n",
    "\n",
    "    import os\n",
    "    from matplotlib import ticker\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Set up matplotlib\n",
    "    SMALL_SIZE = 7\n",
    "    MEDIUM_SIZE = 8\n",
    "    BIGGER_SIZE = 9\n",
    "    plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "    \n",
    "    # Use latex package\n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.rc('text.latex', preamble=r'\\usepackage{bm}')\n",
    "    \n",
    "    # Data directory\n",
    "    fig_dir = './figs/'\n",
    "    \n",
    "    # Figure file\n",
    "    fig_dir = fig_dir \n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(fig_dir):\n",
    "        os.makedirs(fig_dir)\n",
    "    \n",
    "    # Figure version\n",
    "    version = 'v8'\n",
    "    \n",
    "    # Figure file\n",
    "    fig_file = fig_dir + 'paper_figure1_'+version+'.pdf'\n",
    "    \n",
    "    # Create the figure\n",
    "    cm_to_in = 1/2.54\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=3,\n",
    "                            figsize=(17.8*cm_to_in, 9.8*cm_to_in),\n",
    "                            layout=\"constrained\")\n",
    "    \n",
    "    # Define the colors to be used in the plots\n",
    "    colors = ['C0', 'green', 'purple', 'k', 'gray']\n",
    "    \n",
    "    # (a)\n",
    "    plt.sca(axs[0,0])\n",
    "    im = x_true.plot(subplots=False, vmin=-0.6, vmax=0.1, mode='color')\n",
    "    \n",
    "    if physical_dim == 2: \n",
    "        inset_axes = plt.gca().inset_axes([1.04, 0.2, 0.05, 0.6])\n",
    "        fig.colorbar(im[0], ax=plt.gca(), cax=inset_axes)\n",
    "        plt.gca().set_ylim(0, 1)\n",
    "    \n",
    "    #plt.gca().set_xlim(0, 1)\n",
    "    plt.gca().set_title('(a) Exact solution')\n",
    "    plt.ylabel('$\\\\xi^2$')\n",
    "    plt.gca().yaxis.labelpad = -5\n",
    "    plt.xlabel('$\\\\xi^1$')\n",
    "    plt.gca().xaxis.labelpad = -5\n",
    "    \n",
    "    # (b)\n",
    "    plt.sca(axs[0,1])\n",
    "    im = y_true.plot(subplots=False)\n",
    "    \n",
    "    if physical_dim == 2:\n",
    "        inset_axes = plt.gca().inset_axes([1.04, 0.2, 0.05, 0.6])\n",
    "        fig.colorbar(im[0], ax=plt.gca(), cax=inset_axes)\n",
    "        plt.ylabel('$\\\\xi^2$')\n",
    "        plt.gca().yaxis.labelpad = -5\n",
    "        plt.gca().set_title('(b) Exact data')\n",
    "    else:\n",
    "        samples_mean = cuqi.array.CUQIarray(posterior_samples.mean(), geometry=G_domain)\n",
    "        A(samples_mean).plot(subplots=False)\n",
    "        plt.gca().set_title('(b) Exact and predicted data')\n",
    "    plt.xlabel('$\\\\xi^1$')\n",
    "    plt.gca().xaxis.labelpad = -5\n",
    "    \n",
    "    \n",
    "    # (c)\n",
    "    plt.sca(axs[0,2])\n",
    "    im = y_obs.plot(subplots=False)\n",
    "    if physical_dim == 2:\n",
    "        inset_axes = plt.gca().inset_axes([1.04, 0.2, 0.05, 0.6])\n",
    "        fig.colorbar(im[0], ax=plt.gca(), cax=inset_axes)\n",
    "        plt.ylabel('$\\\\xi^2$')\n",
    "        plt.gca().yaxis.labelpad = -5\n",
    "    plt.xlabel('$\\\\xi^1$')\n",
    "    plt.gca().xaxis.labelpad = -5\n",
    "    plt.gca().set_title('(c) Noisy data')\n",
    "    \n",
    "    # (d)\n",
    "    plt.sca(axs[1,0])\n",
    "    im = posterior_samples.plot_mean(\n",
    "        subplots=False, vmin=-0.6, vmax=0.1, mode='color')\n",
    "    if physical_dim == 2:\n",
    "        inset_axes = plt.gca().inset_axes([1.04, 0.2, 0.05, 0.6])\n",
    "        fig.colorbar(im[0], ax=plt.gca(), cax=inset_axes)\n",
    "        plt.gca().set_ylim(0, 1)\n",
    "        plt.ylabel('$\\\\xi^2$')\n",
    "        plt.gca().yaxis.labelpad = -5\n",
    "    \n",
    "    #plt.gca().set_xlim(0, 1)\n",
    "    plt.xlabel('$\\\\xi^1$')\n",
    "    plt.gca().xaxis.labelpad = -5\n",
    "    plt.gca().set_title('(d) Posterior mean')\n",
    "    \n",
    "    # (e)\n",
    "    plt.sca(axs[1,1])\n",
    "    im = posterior_samples.funvals.vector.plot_variance(subplots=False)\n",
    "    \n",
    "    if physical_dim == 2:\n",
    "        inset_axes = plt.gca().inset_axes([1.04, 0.2, 0.05, 0.6])\n",
    "        cb = fig.colorbar(im[0], ax=plt.gca(), cax=inset_axes)\n",
    "        cb.locator = ticker.MaxNLocator(nbins=4)\n",
    "        plt.ylabel('$\\\\xi^2$')\n",
    "        plt.gca().yaxis.labelpad = -5\n",
    "    plt.xlabel('$\\\\xi^1$')\n",
    "    plt.gca().xaxis.labelpad = -5\n",
    "    plt.gca().set_title('(e) Posterior variance')\n",
    "    \n",
    "    # (f)\n",
    "    plt.sca(axs[1,2])\n",
    "    lci = posterior_samples.plot_ci(\n",
    "        97, exact=x_true, plot_par=True, markersize=SMALL_SIZE-3)\n",
    "    lci[0].set_label(\"Mean\")\n",
    "    lci[1].set_label(\"Exact\")\n",
    "    lci[2].set_label(\"$97\\\\%$ CI\")\n",
    "    #plt.ylim(-5, 3)\n",
    "    plt.legend(ncols=2) \n",
    "    plt.ylabel(r'$\\bm{x}_i$')\n",
    "    plt.gca().yaxis.labelpad = -5\n",
    "    plt.gca().yaxis.set_label_coords( -0.06, 0.5)\n",
    "    plt.xlabel('$i$')\n",
    "    plt.gca().xaxis.labelpad = -5\n",
    "    plt.gca().set_title('(f) Posterior CI')\n",
    "    n_ticks = 8\n",
    "    num_var = posterior_samples.geometry.par_dim\n",
    "    tick_ids = np.linspace(0, num_var-1, n_ticks, dtype=int)\n",
    "    plt.xticks(tick_ids, tick_ids)\n",
    "    # switch legend off\n",
    "    plt.gca().legend().set_visible(False)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(fig_file, bbox_inches='tight', pad_inches=0.01, dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_list = posterior_samples.compute_ess()\n",
    "print(ess_list)\n",
    "print(np.min(ess_list))\n",
    "print(np.max(ess_list))\n",
    "print(np.mean(ess_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.plot([-1, -3, -5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "posterior_samples.plot_ci(exact=x_true, plot_par=plot_par)\n",
    "plt.title('Posterior CI (with burnin)')\n",
    "plt.figure()\n",
    "sampler.get_samples().plot_ci(exact=x_true, plot_par=plot_par)\n",
    "plt.title('Posterior CI (without burnin)')\n",
    "plt.figure()\n",
    "mapped_samples = cuqi.samples.Samples( np.exp(posterior_samples.samples))\n",
    "mapped_samples.plot_ci()\n",
    "plt.figure()\n",
    "mapped_samples.plot()\n",
    "\n",
    "plt.figure()\n",
    "samples_geom2 = cuqi.samples.Samples( posterior_samples.samples)\n",
    "samples_geom2.plot_ci()\n",
    "plt.figure()\n",
    "samples_geom2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - [x] 1D case \n",
    "# - [ ] DG0\n",
    "# - [ ] TV with non-MY gradient and with NUTS \n",
    "# - [x] verify the gradient of the posterior\n",
    "# - [x] PCN seems to work (for both 1D and 2D cases)\n",
    "# - [x] Start from x0=the true parameter to debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify gradient  (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(3)\n",
    "# random direction y\n",
    "y0 = np.ones(G_range.par_dim) # np.random.randn(G_range.par_dim)\n",
    "y0[0] = 0\n",
    "y0[-1] = 0\n",
    "\n",
    "# corresponding fenics function\n",
    "y0_fun = dl.Function(G_range.function_space)\n",
    "y0_fun.vector()[:] = y0\n",
    "\n",
    "# objective function\n",
    "def f(x):\n",
    "    return A(x).T@ y0.reshape(-1,1)\n",
    "    #return A(x).vector().get_local().T@ y0.reshape(-1,1)\n",
    "\n",
    "# objective function gradient\n",
    "def fprime(x):\n",
    "    return A.gradient(y0, x)\n",
    "\n",
    "# random input x (the point which gradient is calculated with respect to)\n",
    "x0 = np.random.randn(G_domain.par_dim)\n",
    "\n",
    "# assert that the gradient is correct\n",
    "print(check_grad(f, fprime, x0))\n",
    "\n",
    "plt.plot(fprime(x_true))\n",
    "# plot finite difference gradient \n",
    "\n",
    "plt.plot(approx_gradient(f, x_true, 1e-6), '--')\n",
    "plt.legend(['adjoint based', 'finite difference'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify gradient  (posterior)\n",
    "if physical_dim == 1:\n",
    "    plt.plot(posterior.gradient(x_true))\n",
    "    # plot finite difference gradient \n",
    "\n",
    "    plt.plot(approx_gradient(posterior.logd, x_true, 1e-8), '--')\n",
    "    plt.legend(['adjoint based', 'finite difference'])\n",
    "\n",
    "elif physical_dim == 2:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    plt.sca(axs[0])\n",
    "    grad_fun = dl.Function(H)\n",
    "    grad_fun.vector()[:] = posterior.gradient(x_true.to_numpy())\n",
    "    im = dl.plot(grad_fun, vmin=-12, vmax=6, mode='color')\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    plt.sca(axs[1])\n",
    "    grad_fun_fd = dl.Function(H)\n",
    "    grad_fun_fd.vector()[:] = approx_gradient(posterior.logd, x_true.to_numpy(), 1e-8)\n",
    "    im = dl.plot(grad_fun_fd, vmin=-12, vmax=6, mode='color')\n",
    "    plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the gradient of the prior\n",
    "if physical_dim == 1:\n",
    "    plt.plot(x.gradient(x_true))\n",
    "    # plot finite difference gradient \n",
    "    \n",
    "    plt.plot(approx_gradient(x.logd, x_true, 1e-8), '--')\n",
    "    plt.legend(['adjoint based', 'finite difference'])\n",
    "elif physical_dim == 2:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    plt.sca(axs[0])\n",
    "    grad_fun = dl.Function(H)\n",
    "    grad_fun.vector()[:] = x.gradient(x_true.to_numpy())\n",
    "    im = dl.plot(grad_fun)\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    plt.sca(axs[1])\n",
    "    grad_fun_fd = dl.Function(H)\n",
    "    grad_fun_fd.vector()[:] = approx_gradient(x.logd, x_true.to_numpy(), 1e-8)\n",
    "    im = dl.plot(grad_fun_fd)\n",
    "    plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if physical_dim == 1:\n",
    "    # check the gradient of the likelihood\n",
    "    \n",
    "    plt.plot(posterior.likelihood.gradient(x_true))\n",
    "    # plot finite difference gradient \n",
    "    \n",
    "    plt.plot(approx_gradient(posterior.likelihood.logd, x_true, 1e-10), '--')\n",
    "    plt.legend(['adjoint based', 'finite difference'])\n",
    "elif physical_dim == 2:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    plt.sca(axs[0])\n",
    "    grad_fun = dl.Function(H)\n",
    "    grad_fun.vector()[:] = posterior.likelihood.gradient(x_true.to_numpy())\n",
    "    im = dl.plot(grad_fun)\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    plt.sca(axs[1])\n",
    "    grad_fun_fd = dl.Function(H)\n",
    "    grad_fun_fd.vector()[:] = approx_gradient(posterior.likelihood.logd, x_true.to_numpy(), 1e-8)\n",
    "    im = dl.plot(grad_fun_fd)\n",
    "    plt.colorbar(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 3,
  "vscode": {
   "interpreter": {
    "hash": "f83c72a7c5d885a4a7f43561cb77434137f6f5cf21a7418d4732e18616218db3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
